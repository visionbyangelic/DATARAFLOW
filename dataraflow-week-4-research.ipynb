{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Robust Multimodal Emotion Recognition with Interpretable Fusion\n### A Comparative Study on Missing Modality Adaption\n\n**Author:** Angelic Charles | **ID:** DF2025-069 | **Date:** October 1, 2025\n\n---\n\n## üìù Project Overview\n\nHuman emotion recognition is inherently multimodal, relying on the integration of facial, vocal, and linguistic cues for accurate interpretation. However, existing state-of-the-art systems exhibit significant performance degradation when one or more modalities are missing a common occurrence in real-world environments. This study addresses the challenge of missing modality robustness by proposing an attention-based multimodal fusion architecture optimized for graceful degradation. The framework employs cross-modal transformer fusion with systematic modality dropout training on the CMU-MOSEI dataset, evaluated across six missing modality configurations.\n\n## üîó Project Resources\n\n* **Read the Blog Post:** [**\"Choosing My Research Topic\" on Hashnode**](https://nerdyalgorithm.hashnode.dev/week-4-at-dataraflow-choosing-my-research-topic)\n* **View the Full Research Paper:** [**Complete Paper on OneDrive**](https://1drv.ms/w/c/183b47212b475909/EWRuzmi-vBpEoKyHzp7bF74B-s7JxkcaJ2dYrJEDAwnUjQ?e=kOZLyl)","metadata":{}}]}
